{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe12abc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:32:39.992761Z",
     "iopub.status.busy": "2024-05-02T09:32:39.991984Z",
     "iopub.status.idle": "2024-05-02T09:32:44.347358Z",
     "shell.execute_reply": "2024-05-02T09:32:44.346105Z"
    },
    "papermill": {
     "duration": 4.366131,
     "end_time": "2024-05-02T09:32:44.350201",
     "exception": false,
     "start_time": "2024-05-02T09:32:39.984070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_cols(None)\n",
    "pl.Config.set_tbl_rows(None)\n",
    "pl.enable_string_cache()\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "\n",
    "dataPath = \"C:/Users/Marcel/Documents/Python/kaggle/CreditRiskModel/data/csv_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70184e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:32:44.365936Z",
     "iopub.status.busy": "2024-05-02T09:32:44.365522Z",
     "iopub.status.idle": "2024-05-02T09:32:44.374872Z",
     "shell.execute_reply": "2024-05-02T09:32:44.374020Z"
    },
    "papermill": {
     "duration": 0.020039,
     "end_time": "2024-05-02T09:32:44.377300",
     "exception": false,
     "start_time": "2024-05-02T09:32:44.357261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:  \n",
    "        if df[col].dtype.name in ['object', 'string']:\n",
    "            df[col] = df[col].astype(\"string\").astype('category')\n",
    "            current_categories = df[col].cat.categories\n",
    "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
    "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "            df[col] = df[col].astype(new_dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "\n",
    "    def _numeric_readed_as_string(df: pl.DataFrame, col:str) -> pl.DataFrame:\n",
    "        _missed_boolen = pl.Series([\"false\", 'true']).is_in(df.get_column(col).str.to_lowercase())\n",
    "        if  _missed_boolen[0] == True or _missed_boolen[1] == True:\n",
    "            df = df.with_columns(pl.col(col).str.to_lowercase().str.replace('false',False).str.replace('true', True).cast(pl.Float64).alias(col))\n",
    "        if col[-1] in [\"D\"] or col in [\"date_decision\"]:\n",
    "            df = df.with_columns(pl.col(col).str.replace('\"','').cast(pl.Date).alias(col))\n",
    "        else:\n",
    "            df = df.with_columns(pl.col(col).str.replace('\"','').cast(pl.Float64).alias(col))\n",
    "        return df\n",
    "    \n",
    "    # needed cause set_table_dtypes returns 4 list of columns (bad design)\n",
    "    def _devide_into_type_categories(copy_dtypes: list) -> list:\n",
    "        numeric_col =[]\n",
    "        timedue_col = []\n",
    "        string_col = []\n",
    "        date_col = []\n",
    "        list_names, list_dtypes = copy_dtypes\n",
    "        for name, col_type in zip(list_names,list_dtypes):\n",
    "            if name[-1] in (\"P\", \"A\"): \n",
    "                numeric_col.append(name)\n",
    "            elif name[-1] in (\"T\"):\n",
    "                if col_type == pl.Int64:\n",
    "                    timedue_col.append(name)\n",
    "                else:\n",
    "                     string_col.append(name)\n",
    "            elif name[-1] in (\"L\"):\n",
    "                if col_type == pl.Float64:\n",
    "                    numeric_col.append(name)\n",
    "                else:\n",
    "                    string_col.append(name)\n",
    "            elif name[-1] in (\"D\") or name in [\"date_decision\"]:\n",
    "                date_col.append(name)\n",
    "            elif name[-1] in (\"M\"):\n",
    "                string_col.append(name)\n",
    "        return [numeric_col,timedue_col,string_col,date_col]\n",
    "            \n",
    "\n",
    "    def _paste_dtypes(df: pl.DataFrame, copy_dtypes: tuple) -> pl.DataFrame:\n",
    "        list_names, list_dtypes = copy_dtypes\n",
    "        df = df.select(list_names)\n",
    "        for name, col_type in zip(list_names,list_dtypes):\n",
    "            try:\n",
    "                if df.get_column(name).dtype in [pl.Utf8]:\n",
    "                    df = Preprocess._numeric_readed_as_string(df, name)\n",
    "            except:\n",
    "                pass\n",
    "            df = df.with_columns(pl.col(name).cast(col_type).alias(name))\n",
    "        return df , Preprocess._devide_into_type_categories(copy_dtypes)\n",
    "\n",
    "    def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        numeric_col =[]\n",
    "        timedue_col = []\n",
    "        string_col = []\n",
    "        date_col = []\n",
    "        for col in df.columns:\n",
    "            if col.strip() in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64).alias(col))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                if df.get_column(col).dtype in [pl.Utf8]:\n",
    "                       # for instance \"False\", or '\"2.0\"'\n",
    "                       df = Preprocess._numeric_readed_as_string(df, col)\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "                numeric_col.append(col)\n",
    "            elif col[-1] in (\"T\"):\n",
    "                try:\n",
    "                    if df.get_column(col).dtype in [pl.Utf8]:\n",
    "                       # for instance \"False\", or '\"2.0\"'\n",
    "                       df = Preprocess._numeric_readed_as_string(df, col)\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Int64).alias(col))\n",
    "                    timedue_col.append(col)\n",
    "                except:\n",
    "                     df = df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
    "                     string_col.append(col)\n",
    "            elif col[-1] in (\"L\"):\n",
    "                try:\n",
    "                    if df.get_column(col).dtype in [pl.Utf8]:\n",
    "                       # for instance \"False\", or '\"2.0\"'\n",
    "                       df = Preprocess._numeric_readed_as_string(df, col)\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n",
    "                    numeric_col.append(col)\n",
    "                except:\n",
    "                    df = df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
    "                    string_col.append(col)\n",
    "            elif col[-1] in (\"D\") or col in [\"date_decision\"]:\n",
    "                if df.get_column(col).dtype in [pl.Utf8]:\n",
    "                       # for instance \"False\", or '\"2.0\"'\n",
    "                       df = Preprocess._numeric_readed_as_string(df, col)\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n",
    "                date_col.append(col)\n",
    "            elif col[-1] in (\"M\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
    "                string_col.append(col)\n",
    "        return df, [numeric_col,timedue_col,string_col,date_col]\n",
    "    \n",
    "    def offset_dates_by_decision_date(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        decision_data = df.get_column(\"date_decision\")\n",
    "        for col in df:\n",
    "            if col.name[-1] in [\"D\"]:\n",
    "                df = df.with_columns((col - pl.col(\"date_decision\")).dt.days().cast(pl.Int64).alias(col.name))\n",
    "        df = df.drop(\"date_decision\",\"MONTH\")\n",
    "        return df\n",
    "    \n",
    "    def remove_columns_over_null_limit(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        drop_list = []\n",
    "        for col in df:\n",
    "            if (col.name not in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]) and (col.null_count()/len(col) > 0.7):\n",
    "                drop_list.append(col.name)\n",
    "        df = df.drop(drop_list)\n",
    "        return df, drop_list\n",
    "    \n",
    "    def remove_columns_over_max_cardinality(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        drop_list = []\n",
    "        for col in df:\n",
    "            if (col.name not in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]) and (col.dtype == pl.Utf8) and (col.n_unique() > 200):\n",
    "                drop_list.append(col.name)\n",
    "        df = df.drop(drop_list)\n",
    "        return df, drop_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15ebd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agregator:\n",
    "\n",
    "    def aggreget(df: pl.DataFrame, column_types) -> pl.DataFrame:\n",
    "        numeric_col, timedue_col, string_col, date_col = column_types\n",
    "        \n",
    "        df = df.group_by('case_id').agg(pl.col(numeric_col).sum(), pl.col(timedue_col).mean(),pl.col(string_col).mode(),pl.col(date_col).mean())\n",
    "        for col in string_col:\n",
    "            df = df.with_columns(pl.col(col).list.first().alias(col))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15858b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def _count_files_for_concat(all_files: list) -> dict:\n",
    "        striped_names = []\n",
    "        for file_name in all_files:\n",
    "            suffix = file_name.split('_')[-1]\n",
    "            striped_names.append(file_name.strip(suffix))\n",
    "        count =  Counter(striped_names)\n",
    "        return count\n",
    "\n",
    "    def _column_types_names(files: list) -> list:\n",
    "        files_colname_type = []\n",
    "        for file in files:\n",
    "            column_name = []\n",
    "            column_type = []\n",
    "            for column in file:\n",
    "                if column.name not in ['target']:\n",
    "                    column_name.append(column.name)\n",
    "                    column_type.append(column.dtype)\n",
    "            files_colname_type.append((column_name,column_type))\n",
    "        return files_colname_type\n",
    "    \n",
    "\n",
    "    def load_data_by_depth(paths_list: list, depth: int, copy_load: list = None) -> list:\n",
    "        files_list = []\n",
    "        files_count = Loader._count_files_for_concat(paths_list)\n",
    "        total_path_index = 0\n",
    "        file_number = 0\n",
    "        # loop throught distinct files names and their amount\n",
    "        for file_name, count in files_count.items():\n",
    "            temp_storage_for_concat = []\n",
    "            #read all files which has the same name\n",
    "            for idx in range(count):\n",
    "                file_path = dataPath + paths_list[total_path_index].split('_',2)[0] + '/' + paths_list[total_path_index]\n",
    "                if copy_load == None:\n",
    "                    file, column_types = Preprocess.set_table_dtypes(pl.read_csv(file_path))\n",
    "                else:\n",
    "                    file, column_types = Preprocess._paste_dtypes(pl.read_csv(file_path),copy_load[file_number])\n",
    "                # there are empty files:\n",
    "                if len(file) == 0: \n",
    "                    total_path_index += 1\n",
    "                    continue\n",
    "\n",
    "                if depth > 0 :\n",
    "                    file = Agregator.aggreget(file, column_types)\n",
    "                temp_storage_for_concat.append(file)\n",
    "                total_path_index += 1\n",
    "            file_number +=1\n",
    "            if temp_storage_for_concat:\n",
    "                file = pl.concat(temp_storage_for_concat, how=\"vertical_relaxed\")\n",
    "                file,_ = Preprocess.remove_columns_over_null_limit(file)\n",
    "                file,_ = Preprocess.remove_columns_over_max_cardinality(file)\n",
    "                files_list.append(file)\n",
    "            gc.collect()\n",
    "        return files_list\n",
    "\n",
    "    def load_files(train_paths_dictionary: dict, test_paths_dictionary: dict ) -> list:\n",
    "        train_files = []\n",
    "        test_files = []\n",
    "        for depth in train_paths_dictionary:\n",
    "            train = Loader.load_data_by_depth(train_paths_dictionary[depth], depth)\n",
    "            train_files.append(train)\n",
    "            column_name_type = Loader._column_types_names(train)\n",
    "            test_files.append(Loader.load_data_by_depth(test_paths_dictionary[depth], depth, column_name_type))\n",
    "            print('loaded ', depth)\n",
    "        return train_files,test_files\n",
    "    \n",
    "    def join_tables(list_of_files: list, base_table: pl.DataFrame) -> pl.DataFrame:\n",
    "        for files_on_given_deapth in list_of_files:\n",
    "            for file in files_on_given_deapth:\n",
    "\n",
    "                base_table = base_table.join(\n",
    "                    file, how=\"left\", on=\"case_id\")\n",
    "        return base_table\n",
    "    \n",
    "    def choose_columns_by_availability(train_columns: list, test_columns: list) -> list:\n",
    "        columns = list(set(train_columns) & set(test_columns))\n",
    "        return columns, columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98496251",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = {0: [\"train_static_0_0.csv\",\"train_static_0_1.csv\",\"train_static_cb_0.csv\"],\n",
    "                   1: [\"train_applprev_1_0.csv\",\"train_applprev_1_1.csv\",\"train_other_1.csv\",\"train_tax_registry_a_1.csv\", \"train_tax_registry_b_1.csv\",\"train_tax_registry_c_1.csv\",\n",
    "                              \"train_credit_bureau_a_1_0.csv\",\"train_credit_bureau_a_1_1.csv\",\"train_credit_bureau_a_1_2.csv\",\"train_credit_bureau_a_1_3.csv\",\"train_credit_bureau_b_1.csv\",\n",
    "                              \"train_deposit_1.csv\",\"train_person_1.csv\",\"train_debitcard_1.csv\"],\n",
    "                    2: [\"train_applprev_2.csv\",\"train_person_2.csv\",\"train_credit_bureau_a_2_0.csv\",\"train_credit_bureau_a_2_1.csv\",\"train_credit_bureau_a_2_2.csv\",\"train_credit_bureau_a_2_3.csv\",\n",
    "                             \"train_credit_bureau_a_2_4.csv\",\"train_credit_bureau_a_2_5.csv\",\"train_credit_bureau_a_2_6.csv\",\"train_credit_bureau_a_2_7.csv\",\"train_credit_bureau_a_2_8.csv\",\"train_credit_bureau_a_2_9.csv\",\n",
    "                             \"train_credit_bureau_a_2_10.csv\",\"train_credit_bureau_b_2.csv\"]}\n",
    "\n",
    "data_path_test = {0: [\"test_static_0_0.csv\",\"test_static_0_1.csv\",\"test_static_0_2.csv\",\"test_static_cb_0.csv\"],\n",
    "                  1:  [\"test_applprev_1_0.csv\",\"test_applprev_1_1.csv\",\"test_applprev_1_2.csv\", \"test_other_1.csv\", \"test_tax_registry_a_1.csv\",\"test_tax_registry_b_1.csv\",\"test_tax_registry_c_1.csv\",\n",
    "                              \"test_credit_bureau_a_1_0.csv\",\"test_credit_bureau_a_1_1.csv\",\"test_credit_bureau_a_1_2.csv\",\"test_credit_bureau_a_1_3.csv\",\"test_credit_bureau_a_1_4.csv\",\"test_credit_bureau_b_1.csv\",\n",
    "                              \"test_deposit_1.csv\",\"test_person_1.csv\",\"test_debitcard_1.csv\"],\n",
    "                  2: [\"test_applprev_2.csv\",\"test_person_2.csv\",\"test_credit_bureau_a_2_0.csv\",\"test_credit_bureau_a_2_1.csv\",\"test_credit_bureau_a_2_2.csv\",\"test_credit_bureau_a_2_3.csv\",\n",
    "                             \"test_credit_bureau_a_2_4.csv\",\"test_credit_bureau_a_2_5.csv\",\"test_credit_bureau_a_2_6.csv\",\"test_credit_bureau_a_2_7.csv\",\"test_credit_bureau_a_2_8.csv\",\"test_credit_bureau_a_2_9.csv\",\n",
    "                             \"test_credit_bureau_a_2_10.csv\",\"test_credit_bureau_a_2_11.csv\",\"test_credit_bureau_b_2.csv\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "770e9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded  0\n",
      "loaded  0\n",
      "loaded  1\n",
      "loaded  2\n"
     ]
    }
   ],
   "source": [
    "#load_files returns list for every deapth of files it reads so it has 2 layers of list on top of the pl.dataframe\n",
    "train_basetable, test_basetable = Loader.load_files({0: [\"train_base.csv\"]},{0: [\"test_base.csv\"]})\n",
    "train_files, test_files = Loader.load_files(data_path_train, data_path_test)\n",
    "train_joined = Loader.join_tables(train_files, train_basetable[0][0])\n",
    "test_joined = Loader.join_tables(test_files, test_basetable[0][0])\n",
    "del train_files, test_files, train_basetable, test_basetable\n",
    "gc.collect()\n",
    "train_joined,delete_list = Preprocess.remove_columns_over_null_limit(train_joined)\n",
    "test_joined  = test_joined.drop(delete_list)\n",
    "train_joined = train_joined.pipe(Preprocess.offset_dates_by_decision_date)\n",
    "test_joined = test_joined.pipe(Preprocess.offset_dates_by_decision_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e224182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns, test_columns = Loader.choose_columns_by_availability(train_joined.columns, test_joined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da45cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 4083.6790161132812MB    Test data size: 0.024745941162109375MB\n"
     ]
    }
   ],
   "source": [
    "print(f'Train data size: {train_joined.estimated_size()/1024**2}MB    Test data size: {test_joined.estimated_size()/1024**2}MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc869d9",
   "metadata": {
    "papermill": {
     "duration": 0.006262,
     "end_time": "2024-05-02T09:33:03.292569",
     "exception": false,
     "start_time": "2024-05-02T09:33:03.286307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "In this part, we can see a simple example of joining tables via `case_id`. Here the loading and joining is done with polars library. Polars library is blazingly fast and has much smaller memory footprint than pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2360088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1df87e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:33:05.006908Z",
     "iopub.status.busy": "2024-05-02T09:33:05.006481Z",
     "iopub.status.idle": "2024-05-02T09:33:14.142270Z",
     "shell.execute_reply": "2024-05-02T09:33:14.141026Z"
    },
    "papermill": {
     "duration": 9.146955,
     "end_time": "2024-05-02T09:33:14.145512",
     "exception": false,
     "start_time": "2024-05-02T09:33:04.998557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "case_ids = pd.DataFrame(train_joined[\"case_id\"].unique(), columns=['case_id'])\n",
    "case_ids_train, case_ids_valid = train_test_split(case_ids, train_size=0.8, random_state=1, shuffle=False)\n",
    "case_ids_train = case_ids_train.to_numpy().reshape(-1)\n",
    "case_ids_valid = case_ids_valid.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "def from_polars_to_pandas(df,case_ids: pl.Series, columns_filter) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
    "        df.filter(pl.col(\"case_id\").is_in(case_ids))[columns_filter].to_pandas(),\n",
    "        df.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
    "    )\n",
    "\n",
    "\n",
    "test_joined = test_joined.to_pandas()\n",
    "\n",
    "base_train, X_train, y_train = from_polars_to_pandas(train_joined,case_ids_train,train_columns)\n",
    "base_valid, X_valid, y_valid = from_polars_to_pandas(train_joined,case_ids_valid,train_columns)\n",
    "for df in [X_train,X_valid, test_joined]:\n",
    "    df = convert_strings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b2ffb72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:33:14.160792Z",
     "iopub.status.busy": "2024-05-02T09:33:14.160369Z",
     "iopub.status.idle": "2024-05-02T09:33:14.166379Z",
     "shell.execute_reply": "2024-05-02T09:33:14.165366Z"
    },
    "papermill": {
     "duration": 0.016347,
     "end_time": "2024-05-02T09:33:14.168666",
     "exception": false,
     "start_time": "2024-05-02T09:33:14.152319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1221327, 286)\n",
      "Valid: (305332, 286)\n",
      "Test: (10, 286)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Valid: {X_valid.shape}\")\n",
    "print(f\"Test: {test_joined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c3e5f",
   "metadata": {
    "papermill": {
     "duration": 0.006475,
     "end_time": "2024-05-02T09:33:14.182226",
     "exception": false,
     "start_time": "2024-05-02T09:33:14.175751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training LightGBM\n",
    "\n",
    "Minimal example of LightGBM training is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e421dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:33:14.198296Z",
     "iopub.status.busy": "2024-05-02T09:33:14.197763Z",
     "iopub.status.idle": "2024-05-02T09:34:33.798053Z",
     "shell.execute_reply": "2024-05-02T09:34:33.797006Z"
    },
    "papermill": {
     "duration": 79.611628,
     "end_time": "2024-05-02T09:34:33.800497",
     "exception": false,
     "start_time": "2024-05-02T09:33:14.188869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.745614\n",
      "[100]\tvalid_0's auc: 0.769889\n",
      "[150]\tvalid_0's auc: 0.781988\n",
      "[200]\tvalid_0's auc: 0.788971\n",
      "[250]\tvalid_0's auc: 0.792884\n",
      "[300]\tvalid_0's auc: 0.797546\n",
      "[350]\tvalid_0's auc: 0.801184\n",
      "[400]\tvalid_0's auc: 0.803905\n",
      "[450]\tvalid_0's auc: 0.805291\n",
      "Early stopping, best iteration is:\n",
      "[463]\tvalid_0's auc: 0.806628\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 3,\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=lgb_valid,\n",
    "    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ff3fd",
   "metadata": {
    "papermill": {
     "duration": 0.007961,
     "end_time": "2024-05-02T09:34:33.816880",
     "exception": false,
     "start_time": "2024-05-02T09:34:33.808919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluation with AUC and then comparison with the stability metric is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2552d02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:34:33.835739Z",
     "iopub.status.busy": "2024-05-02T09:34:33.834654Z",
     "iopub.status.idle": "2024-05-02T09:34:55.638748Z",
     "shell.execute_reply": "2024-05-02T09:34:55.637906Z"
    },
    "papermill": {
     "duration": 21.815783,
     "end_time": "2024-05-02T09:34:55.640973",
     "exception": false,
     "start_time": "2024-05-02T09:34:33.825190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score on the train set is: 0.8391168516079585\n",
      "The AUC score on the valid set is: 0.8066282100832387\n"
     ]
    }
   ],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid)]:\n",
    "    y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \n",
    "print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f61af21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:34:55.660345Z",
     "iopub.status.busy": "2024-05-02T09:34:55.659645Z",
     "iopub.status.idle": "2024-05-02T09:34:56.732840Z",
     "shell.execute_reply": "2024-05-02T09:34:56.731339Z"
    },
    "papermill": {
     "duration": 1.085972,
     "end_time": "2024-05-02T09:34:56.735415",
     "exception": false,
     "start_time": "2024-05-02T09:34:55.649443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stability score on the train set is: 0.6424208964868618 a = 0.00011886247492601548 avg_gini = 0.6644752351829734 res_std = 0.044108677392222975\n",
      "The stability score on the valid set is: 0.54664618678408 a = 0.003252229519540812 avg_gini = 0.5790713352144955 res_std = 0.06485029686083121\n"
     ]
    }
   ],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std, a, avg_gini, res_std\n",
    "\n",
    "stability_score_train, a_train, avg_gini_train, res_std_train = gini_stability(base_train)\n",
    "stability_score_valid, a_valid, avg_gini_valid, res_std_valid = gini_stability(base_valid)\n",
    "\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train} a = {a_train} avg_gini = {avg_gini_train} res_std = {res_std_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid} a = {a_valid} avg_gini = {avg_gini_valid} res_std = {res_std_valid}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d18024",
   "metadata": {
    "papermill": {
     "duration": 0.008411,
     "end_time": "2024-05-02T09:34:56.752764",
     "exception": false,
     "start_time": "2024-05-02T09:34:56.744353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission\n",
    "\n",
    "Scoring the submission dataset is below, we need to take care of new categories. Then we save the score as a last step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b254092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:34:56.772290Z",
     "iopub.status.busy": "2024-05-02T09:34:56.771827Z",
     "iopub.status.idle": "2024-05-02T09:34:56.878768Z",
     "shell.execute_reply": "2024-05-02T09:34:56.877894Z"
    },
    "papermill": {
     "duration": 0.119466,
     "end_time": "2024-05-02T09:34:56.881283",
     "exception": false,
     "start_time": "2024-05-02T09:34:56.761817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_submission = test_joined[train_columns]\n",
    "X_submission = convert_strings(X_submission)\n",
    "\n",
    "y_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07deeb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T09:34:56.900699Z",
     "iopub.status.busy": "2024-05-02T09:34:56.900031Z",
     "iopub.status.idle": "2024-05-02T09:34:56.911959Z",
     "shell.execute_reply": "2024-05-02T09:34:56.911054Z"
    },
    "papermill": {
     "duration": 0.024201,
     "end_time": "2024-05-02T09:34:56.914247",
     "exception": false,
     "start_time": "2024-05-02T09:34:56.890046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"case_id\": test_joined[\"case_id\"].to_numpy(),\n",
    "    \"score\": y_submission_pred\n",
    "}).set_index('case_id')\n",
    "submission.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 142.51015,
   "end_time": "2024-05-02T09:34:58.164264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-02T09:32:35.654114",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
